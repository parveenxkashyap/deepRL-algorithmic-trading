{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91bda495",
   "metadata": {},
   "source": [
    "# Algorithmic Trading Using Deep Reinforcdment Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe2a663",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "- [Introduction](#scrollTo=9SNR5Z82unXd)\n",
    "\n",
    "- [Import Dependencies](#scrollTo=012Sf4GHumaL)\n",
    "\n",
    "- [Data & Preprocessing](#scrollTo=fAq9RY7dtwTe)\n",
    "\n",
    "- [Custom Trading Environment Setting](#scrollTo=z-g6RJHLpuKh)\n",
    "\n",
    "- [Utility Functions](#scrollTo=jWElTzIctZ3E)\n",
    "\n",
    "- [PPO Agent](#scrollTo=rz1CA85UkXPI)\n",
    "\n",
    "  - [Agent setting](#scrollTo=bKxYFvx8nALx)\n",
    "\n",
    "  - [Training](#scrollTo=VXm3OnAlnF51)\n",
    "\n",
    "  - [Results and Validation](#scrollTo=3D6ypjL-nh8J)\n",
    "\n",
    "- [DQN Agent](#scrollTo=F1SfonB9kXPR)\n",
    "\n",
    "  - [Agent Setting](#scrollTo=hIqzHqzPn2Lv)\n",
    "\n",
    "  - [Training](#scrollTo=6tBBq8TQn7JZ)\n",
    "\n",
    "  - [Results and Validation](#scrollTo=SukTkoX8oExx)\n",
    "\n",
    "- [Visualization](#scrollTo=V48fsZn7jv9Q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900532b2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8b19d7",
   "metadata": {},
   "source": [
    "In quantitative finance, stock trading is essentially a dynamic decision problem, that is, deciding where, at what price, and how much to trade in a highly stochastic, dynamic, and complex stock market. With recent advances in deep reinforcement learning (DRL) methods, sequential dynamic decision problems can be modeled and solved with a human-like approach.\n",
    "\n",
    "<br>\n",
    "\n",
    "In this poject, we examine the potential and performance of deep reinforcement learning to optimize stock trading strategies and thus maximize investment returns. Google stock is selected as our trading stock and the daily opening and closing price along with trading volume and several technical indicators are used as a training environment and trading market.\n",
    "\n",
    "<br>\n",
    "\n",
    "We present two trading agents based on deep reinforcement learning, one using Proximal Policy Otimization algorithm and the other based on Deep Q-Learing, to autonomously make trading decisions and generate returns in dynamic financial markets. The performance of these intelligent agents is compared with the performance of the buy and hold strategy. And at the end, it is shown that the proposed deep reinforcement learning approach performs better than the buy and hold benchmark in terms of risk assessment criteria and portfolio return.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**References:**\n",
    "* Human-level control through deep reinforcement learning (Deep Q-Learning) : [paper](https://www.nature.com/articles/nature14236)\n",
    "* Proximal Policy Optimization) : [paper](https://arxiv.org/abs/1707.06347), [blog](https://openai.com/blog/openai-baselines-ppo/), [spinning-up](https://spinningup.openai.com/en/latest/algorithms/ppo.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bcf1d7",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install talib-binary\n",
    "!pip install gym_anytrading\n",
    "!pip install quantstats\n",
    "!pip install stable_baselines3\n",
    "!pip install pyfolio\n",
    "!pip install --upgrade gym==0.25.2\n",
    "!pip install stable_baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc973b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import talib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t\n",
    "from pandas_datareader import data as web\n",
    "import pandas_datareader as pdr\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm import tqdm\n",
    "from gym.utils import seeding\n",
    "import gym\n",
    "from gym import spaces\n",
    "import logging\n",
    "import datetime\n",
    "import pyfolio.timeseries as ts\n",
    "import scipy.stats as st\n",
    "\n",
    "from gym_anytrading.envs import TradingEnv, ForexEnv, StocksEnv, Actions, Positions \n",
    "# from gym_anytrading.datasets import FOREX_EURUSD_1H_ASK, STOCKS_GOOGL\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import quantstats as qs\n",
    "\n",
    "from stable_baselines3 import A2C, DDPG, DQN, PPO, TD3, SAC\n",
    "# from stable_baselines import TRPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "# import torch\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "\n",
    "import torch as th\n",
    "from torch import nn\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "DECIMAL_SIGNS = 5\n",
    "rnd = lambda x: round(x, DECIMAL_SIGNS)\n",
    "\n",
    "#==========="
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
